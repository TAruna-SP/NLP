{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrDWdcjHbjAEhlGCpCOSzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAruna-SP/NLP/blob/week-1/Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we saw Lemmatization which is advanced compared to stemming.\n",
        "\n",
        "*   Lemmatization cuts the word at the same time keeps its meaning. eg. computers-> computer ( stemming results as 'compute' - non sense)\n"
      ],
      "metadata": {
        "id": "WAIOW_kda6LX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbXZLCMrWIth",
        "outputId": "08186e5e-8446-4b71-e41a-544b6b8fb503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloads complete. Lemmatizer ready.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"Downloads complete. Lemmatizer ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the filtered_words from Day 3\n",
        "# Let's make sure we have the list. If not, re-create it:\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = ['natural', 'language', 'processing', 'nlp', 'amazing', 'helps', 'computers', 'understand', 'human', 'language', 'think', 'fascinating']\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(w) for w in filtered_words]\n",
        "\n",
        "print(\"The Problem with Stemming:\")\n",
        "for original, stem in zip(filtered_words, stemmed):\n",
        "    print(f\"  {original:15} -> {stem:15}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGLUolAHX_tF",
        "outputId": "06055074-2f9c-40e2-83f4-5d2ccd3739d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Problem with Stemming:\n",
            "  natural         -> natur          \n",
            "  language        -> languag        \n",
            "  processing      -> process        \n",
            "  nlp             -> nlp            \n",
            "  amazing         -> amaz           \n",
            "  helps           -> help           \n",
            "  computers       -> comput         \n",
            "  understand      -> understand     \n",
            "  human           -> human          \n",
            "  language        -> languag        \n",
            "  think           -> think          \n",
            "  fascinating     -> fascin         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic lemmatization (treats every word as a noun)\n",
        "lemmatized_noun = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "print(\"Basic Lemmatization (as Nouns):\")\n",
        "for original, lemma in zip(filtered_words, lemmatized_noun):\n",
        "    print(f\"  {original:15} -> {lemma:15}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z74sqNBCYEcv",
        "outputId": "17ccebcd-da56-4302-f2c6-5dc664088d26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Lemmatization (as Nouns):\n",
            "  natural         -> natural        \n",
            "  language        -> language       \n",
            "  processing      -> processing     \n",
            "  nlp             -> nlp            \n",
            "  amazing         -> amazing        \n",
            "  helps           -> help           \n",
            "  computers       -> computer       \n",
            "  understand      -> understand     \n",
            "  human           -> human          \n",
            "  language        -> language       \n",
            "  think           -> think          \n",
            "  fascinating     -> fascinating    \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "# First, tag each word with its POS\n",
        "tagged_words = pos_tag(filtered_words)\n",
        "print(\"Part-of-Speech Tags:\")\n",
        "print(tagged_words)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IlWEBhQYQCe",
        "outputId": "b56b52cf-c0f5-4ea6-bb93-803e3062bb6c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part-of-Speech Tags:\n",
            "[('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'JJ'), ('amazing', 'NN'), ('helps', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('think', 'VBP'), ('fascinating', 'VBG')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Convert POS tag to WordNet format.\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default to noun\n",
        "\n",
        "# Now lemmatize each word with its correct POS\n",
        "lemmatized_correct = []\n",
        "for word, tag in tagged_words:\n",
        "    wordnet_pos = get_wordnet_pos(tag)\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
        "    lemmatized_correct.append(lemma)\n",
        "\n",
        "print(\"Advanced Lemmatization (with correct POS):\")\n",
        "for original, tag, lemma in zip(filtered_words, [t for w,t in tagged_words], lemmatized_correct):\n",
        "    print(f\"  {original:15} ({tag:5}) -> {lemma:15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxyDdmv_YXUJ",
        "outputId": "42baf1f1-7cb7-4afc-f266-0e82aadb89ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced Lemmatization (with correct POS):\n",
            "  natural         (JJ   ) -> natural        \n",
            "  language        (NN   ) -> language       \n",
            "  processing      (NN   ) -> processing     \n",
            "  nlp             (JJ   ) -> nlp            \n",
            "  amazing         (NN   ) -> amazing        \n",
            "  helps           (VBZ  ) -> help           \n",
            "  computers       (NNS  ) -> computer       \n",
            "  understand      (VBP  ) -> understand     \n",
            "  human           (JJ   ) -> human          \n",
            "  language        (NN   ) -> language       \n",
            "  think           (VBP  ) -> think          \n",
            "  fascinating     (VBG  ) -> fascinate      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== FINAL COMPARISON ===\")\n",
        "print(f\"{'Word':<15} | {'Stem':<10} | {'Lemma (Smart)':<15}\")\n",
        "print(\"-\" * 45)\n",
        "for i in range(len(filtered_words)):\n",
        "    print(f\"{filtered_words[i]:<15} | {stemmed[i]:<10} | {lemmatized_correct[i]:<15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPVJuFsxYnlo",
        "outputId": "62ed2589-62c2-4483-b795-7fcb6f824c37"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL COMPARISON ===\n",
            "Word            | Stem       | Lemma (Smart)  \n",
            "---------------------------------------------\n",
            "natural         | natur      | natural        \n",
            "language        | languag    | language       \n",
            "processing      | process    | processing     \n",
            "nlp             | nlp        | nlp            \n",
            "amazing         | amaz       | amazing        \n",
            "helps           | help       | help           \n",
            "computers       | comput     | computer       \n",
            "understand      | understand | understand     \n",
            "human           | human      | human          \n",
            "language        | languag    | language       \n",
            "think           | think      | think          \n",
            "fascinating     | fascin     | fascinate      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_FA5x-tY1bm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
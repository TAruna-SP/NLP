{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZhnZLiNc7AyTh7X4/hgE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAruna-SP/NLP/blob/week-1/Tokenization_with_NLTK_library_vs_Split_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start this journey using NLTK library.\n",
        "* we saw tokenization using word_tokenize() and sent_tokenize() functions\n",
        "* word_tokenize() splits the sentences into words.\n",
        "* sent_tokenize() splits colletion of sentences into individual sentences.\n",
        "\n",
        "** tokenizing a sentence can be achieved using split() method from string as well.\n",
        "** But the key difference btw the two is split() keeps the punctuation along with the word as it is like \"It'll\" while word_tokenize() splits it into word and punctuation marks as two separate tokens like \"It\" and \"'ll\".\n",
        "if there is '.', word_tokenize create treat it as a seperate token."
      ],
      "metadata": {
        "id": "I1DEvsGVhP7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use a fresh sample text for Day 2\n",
        "text = \"Dr. Turing and his colleagues are developing AI. They think it'll change the world by 2030. Let's watch!\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# 1. Split into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Step 1 - Sentences:\")\n",
        "print(sentences)\n",
        "print() # Adds an empty line for readability\n",
        "\n",
        "# 2. Split into words (using the original text)\n",
        "words = word_tokenize(text)\n",
        "print(\"Step 2 - All Words:\")\n",
        "print(words)\n",
        "print(f\"   Total word count: {len(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paG_qnP5Q-U_",
        "outputId": "0a38db4c-12ab-4266-efb5-5196767ab7b3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 - Sentences:\n",
            "['Dr. Turing and his colleagues are developing AI.', \"They think it'll change the world by 2030.\", \"Let's watch!\"]\n",
            "\n",
            "Step 2 - All Words:\n",
            "['Dr.', 'Turing', 'and', 'his', 'colleagues', 'are', 'developing', 'AI', '.', 'They', 'think', 'it', \"'ll\", 'change', 'the', 'world', 'by', '2030', '.', 'Let', \"'s\", 'watch', '!']\n",
            "   Total word count: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare with Python's basic split\n",
        "naive_words = text.split()\n",
        "print(\"Step 3 - Comparison:\")\n",
        "print(f\"word_tokenize: {words}\")\n",
        "print(f\"text.split():  {naive_words}\")\n",
        "print()\n",
        "print(\"Key differences to notice:\")\n",
        "print(\"- word_tokenize keeps punctuation as separate tokens (e.g., '.', '!').\")\n",
        "print(\"- It correctly handles contractions like \\\"it'll\\\" and \\\"Let's\\\".\")\n",
        "print(\"- It treats 'Dr.' as a single token, understanding it's an abbreviation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fdCkMSIRGQ-",
        "outputId": "e595a68e-e764-4622-f372-8f2edefbde30"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3 - Comparison:\n",
            "word_tokenize: ['Dr.', 'Turing', 'and', 'his', 'colleagues', 'are', 'developing', 'AI', '.', 'They', 'think', 'it', \"'ll\", 'change', 'the', 'world', 'by', '2030', '.', 'Let', \"'s\", 'watch', '!']\n",
            "text.split():  ['Dr.', 'Turing', 'and', 'his', 'colleagues', 'are', 'developing', 'AI.', 'They', 'think', \"it'll\", 'change', 'the', 'world', 'by', '2030.', \"Let's\", 'watch!']\n",
            "\n",
            "Key differences to notice:\n",
            "- word_tokenize keeps punctuation as separate tokens (e.g., '.', '!').\n",
            "- It correctly handles contractions like \"it'll\" and \"Let's\".\n",
            "- It treats 'Dr.' as a single token, understanding it's an abbreviation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR TURN: Try your own text\n",
        "my_text = \"Write any sentence you like here, maybe about AI or your favorite food!\"\n",
        "my_words = word_tokenize(my_text)\n",
        "print(\"My tokenized words:\", my_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_-xPjssRLi6",
        "outputId": "1d722a01-1aac-4875-8e1f-899c1fef86a8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My tokenized words: ['Write', 'any', 'sentence', 'you', 'like', 'here', ',', 'maybe', 'about', 'AI', 'or', 'your', 'favorite', 'food', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import string\n",
        "\n",
        "print(\"=== Welcome to the Text Inspector ===\\n\")\n",
        "\n",
        "# 1. Input\n",
        "user_text = \"Natural Language Processing (NLP) is amazing! It helps computers understand human language. Don't you think it's fascinating?\"\n",
        "\n",
        "# 2. Process\n",
        "words = word_tokenize(user_text)\n",
        "sentences = sent_tokenize(user_text)\n",
        "\n",
        "# Clean words: lowercase and remove pure punctuation tokens\n",
        "clean_words = [w.lower() for w in words if w not in string.punctuation]\n",
        "\n",
        "# 3. Analyze\n",
        "word_count = len(clean_words)\n",
        "sentence_count = len(sentences)\n",
        "unique_words = len(set(clean_words))  # set() removes duplicates\n",
        "avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0\n",
        "\n",
        "# 4. Report\n",
        "print(\"Original Text:\")\n",
        "print(f'\"{user_text}\"\\n')\n",
        "print(\"--- Inspection Report ---\")\n",
        "print(f\"1. Sentences: {sentence_count}\")\n",
        "print(f\"2. Total Words: {word_count} (Unique: {unique_words})\")\n",
        "print(f\"3. Avg. Words/Sentence: {avg_words_per_sentence:.1f}\")\n",
        "print(f\"4. Top 5 Frequent Words:\")\n",
        "from collections import Counter\n",
        "for word, freq in Counter(clean_words).most_common(5):\n",
        "    print(f\"   '{word}' appears {freq} time(s)\")\n",
        "\n",
        "print(\"\\n=== Inspection Complete ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBCKyG5o4qQz",
        "outputId": "9662eb02-1474-4f7a-e1a4-989cf4017007"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Welcome to the Text Inspector ===\n",
            "\n",
            "Original Text:\n",
            "\"Natural Language Processing (NLP) is amazing! It helps computers understand human language. Don't you think it's fascinating?\"\n",
            "\n",
            "--- Inspection Report ---\n",
            "1. Sentences: 3\n",
            "2. Total Words: 19 (Unique: 17)\n",
            "3. Avg. Words/Sentence: 6.3\n",
            "4. Top 5 Frequent Words:\n",
            "   'language' appears 2 time(s)\n",
            "   'it' appears 2 time(s)\n",
            "   'natural' appears 1 time(s)\n",
            "   'processing' appears 1 time(s)\n",
            "   'nlp' appears 1 time(s)\n",
            "\n",
            "=== Inspection Complete ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_o02DlCgaTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
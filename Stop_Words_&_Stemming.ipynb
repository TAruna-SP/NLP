{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONly0JVyCz8Ym7WfSigC/q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAruna-SP/NLP/blob/week-1/Stop_Words_%26_Stemming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUCMH_K3tS4s",
        "outputId": "f963acca-a4f3-446c-9794-2ab73955d477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A sample of English stopwords: ['now', 'be', 't', 'there', 'him', 'your', 'as', \"it's\", 'itself', 'wasn']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Get the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(\"A sample of English stopwords:\", list(stop_words)[:10])\n",
        "print() # Empty line"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-use our text from the Micro-Project\n",
        "user_text = \"Natural Language Processing (NLP) is amazing! It helps computers understand human language. Don't you think it's fascinating?\"\n",
        "\n",
        "# 1. Tokenize and clean (as before)\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "words = word_tokenize(user_text)\n",
        "clean_words = [w.lower() for w in words if w not in string.punctuation]\n",
        "\n",
        "# 2. NEW: Remove stopwords\n",
        "filtered_words = [w for w in clean_words if w not in stop_words]\n",
        "print(\"Step 1 - Removing Stopwords:\")\n",
        "print(f\"Original words ({len(clean_words)}): {clean_words}\")\n",
        "print(f\"Filtered words ({len(filtered_words)}): {filtered_words}\")\n",
        "print() # Empty line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POQPSpQRuqRw",
        "outputId": "e4851116-7a28-4b52-fe92-a0ce09accb19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 - Removing Stopwords:\n",
            "Original words (19): ['natural', 'language', 'processing', 'nlp', 'is', 'amazing', 'it', 'helps', 'computers', 'understand', 'human', 'language', 'do', \"n't\", 'you', 'think', 'it', \"'s\", 'fascinating']\n",
            "Filtered words (14): ['natural', 'language', 'processing', 'nlp', 'amazing', 'helps', 'computers', 'understand', 'human', 'language', \"n't\", 'think', \"'s\", 'fascinating']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. NEW: Apply stemming to the filtered list\n",
        "stemmed_words = [stemmer.stem(w) for w in filtered_words]\n",
        "print(\"Step 2 - Applying Stemming:\")\n",
        "print(f\"Filtered words: {filtered_words}\")\n",
        "print(f\"Stemmed words : {stemmed_words}\")\n",
        "print() # Empty line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQJyZDQpvjY8",
        "outputId": "4e890073-09a2-459f-d640-27e40724724d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2 - Applying Stemming:\n",
            "Filtered words: ['natural', 'language', 'processing', 'nlp', 'amazing', 'helps', 'computers', 'understand', 'human', 'language', \"n't\", 'think', \"'s\", 'fascinating']\n",
            "Stemmed words : ['natur', 'languag', 'process', 'nlp', 'amaz', 'help', 'comput', 'understand', 'human', 'languag', \"n't\", 'think', \"'s\", 'fascin']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Text Inspector 2.0 (With Stopwords & Stemming) ===\\n\")\n",
        "# Analyze the FILTERED list (more meaningful)\n",
        "from collections import Counter\n",
        "word_freq = Counter(filtered_words)\n",
        "\n",
        "print(\"Top 5 Meaningful Words:\")\n",
        "for word, freq in word_freq.most_common(5):\n",
        "    print(f\"   '{word}' appears {freq} time(s)\")\n",
        "\n",
        "# Bonus: See the effect of stemming on frequency\n",
        "stemmed_freq = Counter(stemmed_words)\n",
        "print(\"\\nTop 5 Stems (grouping similar words):\")\n",
        "for stem, freq in stemmed_freq.most_common(5):\n",
        "    print(f\"   '{stem}' appears {freq} time(s)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-cLoVovwlI8",
        "outputId": "d0f28ceb-004e-4795-d0f6-ed9460f62031"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Text Inspector 2.0 (With Stopwords & Stemming) ===\n",
            "\n",
            "Top 5 Meaningful Words:\n",
            "   'language' appears 2 time(s)\n",
            "   'natural' appears 1 time(s)\n",
            "   'processing' appears 1 time(s)\n",
            "   'nlp' appears 1 time(s)\n",
            "   'amazing' appears 1 time(s)\n",
            "\n",
            "Top 5 Stems (grouping similar words):\n",
            "   'languag' appears 2 time(s)\n",
            "   'natur' appears 1 time(s)\n",
            "   'process' appears 1 time(s)\n",
            "   'nlp' appears 1 time(s)\n",
            "   'amaz' appears 1 time(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnMtmz6GwsHd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
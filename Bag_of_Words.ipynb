{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdYFH/CNPtSU1tsJDu7T8h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAruna-SP/NLP/blob/week-1/Bag_of_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words is a model in nlp which turns a sentence/sentences into collection of words and keep track of the frequency of each word in every sentence.THis way, a text is converted to numbers for machine learning model to process."
      ],
      "metadata": {
        "id": "qrBSR7stRcv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oOHyxOIxc_F",
        "outputId": "1dda53dc-30c7-4bef-8683-fda7b8878028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Corpus:\n",
            "Doc 1: I love machine learning\n",
            "Doc 2: Machine learning is great\n",
            "Doc 3: I love coding in Python\n",
            "\n",
            "Step 1 - Vocabulary (9 words): ['coding', 'great', 'i', 'in', 'is', 'learning', 'love', 'machine', 'python']\n",
            "\n",
            "Step 2 - Manual Bag-of-Words Vectors:\n",
            "'I love machine learning' -> [0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
            "'Machine learning is great' -> [0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
            "'I love coding in Python' -> [1, 0, 1, 1, 0, 0, 1, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "# Our small corpus (collection of documents)\n",
        "documents = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is great\",\n",
        "    \"I love coding in Python\"\n",
        "]\n",
        "print(\"Our Corpus:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Doc {i+1}: {doc}\")\n",
        "\n",
        "# Step 1: Create the vocabulary (unique words)\n",
        "vocabulary = set()\n",
        "for doc in documents:\n",
        "    for word in doc.lower().split(): # Simple split for now\n",
        "        vocabulary.add(word)\n",
        "vocabulary = sorted(list(vocabulary)) # Sort for consistency\n",
        "print(f\"\\nStep 1 - Vocabulary ({len(vocabulary)} words): {vocabulary}\")\n",
        "\n",
        "# Step 2: Create BoW vectors manually\n",
        "print(\"\\nStep 2 - Manual Bag-of-Words Vectors:\")\n",
        "for doc in documents:\n",
        "    # Create a vector of zeros with same length as vocabulary\n",
        "    vector = [0] * len(vocabulary)\n",
        "    words = doc.lower().split()\n",
        "    for word in words:\n",
        "        # Find the index of this word in vocabulary and increment count\n",
        "        index = vocabulary.index(word)\n",
        "        vector[index] += 1\n",
        "    print(f\"'{doc}' -> {vector}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install scikit-learn if you haven't\n",
        "!pip install scikit-learn -q\n",
        "print(\"sklearn installed/verified.\")\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd # For nice table display\n",
        "\n",
        "# Re-create our simple corpus\n",
        "documents = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is great\",\n",
        "    \"I love coding in Python\"\n",
        "]\n",
        "\n",
        "# Create and fit the vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents) # 'X' is the standard name for the feature matrix\n",
        "\n",
        "# Investigate the results\n",
        "print(\"\\n--- Professional Bag-of-Words with scikit-learn ---\")\n",
        "print(f\"\\n1. Vocabulary (Feature Names):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(f\"\\n2. Dense Matrix Representation (Documents x Words):\")\n",
        "# Convert from sparse matrix to dense array for display\n",
        "dense_array = X.toarray()\n",
        "print(dense_array)\n",
        "\n",
        "print(f\"\\n3. As a Readable DataFrame:\")\n",
        "df_bow = pd.DataFrame(dense_array,\n",
        "                     columns=vectorizer.get_feature_names_out(),\n",
        "                     index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "print(df_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH0mvSIjzPfG",
        "outputId": "c84b60cb-b15c-4d33-8846-e4cc3b31e7d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn installed/verified.\n",
            "\n",
            "--- Professional Bag-of-Words with scikit-learn ---\n",
            "\n",
            "1. Vocabulary (Feature Names):\n",
            "['coding' 'great' 'in' 'is' 'learning' 'love' 'machine' 'python']\n",
            "\n",
            "2. Dense Matrix Representation (Documents x Words):\n",
            "[[0 0 0 0 1 1 1 0]\n",
            " [0 1 0 1 1 0 1 0]\n",
            " [1 0 1 0 0 1 0 1]]\n",
            "\n",
            "3. As a Readable DataFrame:\n",
            "       coding  great  in  is  learning  love  machine  python\n",
            "Doc 1       0      0   0   0         1     1        1       0\n",
            "Doc 2       0      1   0   1         1     0        1       0\n",
            "Doc 3       1      0   1   0         0     1        0       1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Enhanced BoW with Our NLP Tools ---\")\n",
        "# We can customize CountVectorizer to use our own tokenizer and stop words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure NLTK data is available\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define a custom tokenizer function\n",
        "def custom_tokenizer(text):\n",
        "    # 1. Tokenize using NLTK\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # 2. Remove stopwords and single-character tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered = [w for w in tokens if w not in stop_words and len(w) > 1]\n",
        "    return filtered\n",
        "\n",
        "# Create vectorizer with our custom tokenizer\n",
        "enhanced_vectorizer = CountVectorizer(tokenizer=custom_tokenizer)\n",
        "X_enhanced = enhanced_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Vocabulary (after removing stopwords like 'i', 'is', 'in'):\")\n",
        "print(enhanced_vectorizer.get_feature_names_out())\n",
        "\n",
        "df_enhanced = pd.DataFrame(X_enhanced.toarray(),\n",
        "                           columns=enhanced_vectorizer.get_feature_names_out(),\n",
        "                           index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "print(\"\\nEnhanced BoW DataFrame:\")\n",
        "print(df_enhanced)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF4rYHAF0qJR",
        "outputId": "d1c09af7-1877-41a4-fedf-76b53b938b13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Enhanced BoW with Our NLP Tools ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (after removing stopwords like 'i', 'is', 'in'):\n",
            "['coding' 'great' 'learning' 'love' 'machine' 'python']\n",
            "\n",
            "Enhanced BoW DataFrame:\n",
            "       coding  great  learning  love  machine  python\n",
            "Doc 1       0      0         1     1        1       0\n",
            "Doc 2       0      1         1     0        1       0\n",
            "Doc 3       1      0         0     1        0       1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TvUDAQJwRpag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}